{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "complete_pipe_line_of_QA_and_stance_detection modified.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "223CFlKjr3Qb",
        "colab_type": "text"
      },
      "source": [
        "**Project Codes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9liNOG_6Grb",
        "colab_type": "code",
        "outputId": "1bf4c4fb-3c08-4520-ea22-5238486f9b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install newspaper3k\n",
        "!pip install spacy==2.1.0\n",
        "!pip install neuralcoref\n",
        "!python -m spacy download en_core_web_lg #you will need to install this on first load\n",
        "!pip install cdqa"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.21.0)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.2.2)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (5.2.1)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.6.1)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.1->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (41.4.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=3.3.0->newspaper3k) (0.46)\n",
            "Requirement already satisfied: spacy==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.3.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.2.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (7.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.0.2)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (1.17.3)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.0) (0.9.6)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.28.1)\n",
            "Requirement already satisfied: neuralcoref in /usr/local/lib/python3.6/dist-packages (4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.17.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.10.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.21.0)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.1.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.7 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (1.13.7)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2019.9.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.3.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->neuralcoref) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->neuralcoref) (2.6.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.28.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.7->boto3->neuralcoref) (1.12.0)\n",
            "Requirement already satisfied: en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Requirement already satisfied: cdqa in /usr/local/lib/python3.6/dist-packages (1.3.5)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from cdqa) (1.1.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.6/dist-packages (from cdqa) (0.7.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from cdqa) (0.25.3)\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (from cdqa) (0.6.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.6/dist-packages (from cdqa) (3.1.1)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.6/dist-packages (from cdqa) (3.0.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from cdqa) (4.28.1)\n",
            "Requirement already satisfied: tika in /usr/local/lib/python3.6/dist-packages (from cdqa) (1.19)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (from cdqa) (3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from cdqa) (0.21.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from cdqa) (0.14.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->cdqa) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->cdqa) (2.10.3)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->cdqa) (0.16.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask->cdqa) (7.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->cdqa) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->cdqa) (1.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->cdqa) (2.6.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert->cdqa) (1.3.0+cu100)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert->cdqa) (1.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert->cdqa) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert->cdqa) (2019.11.1)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.6/dist-packages (from markdown->cdqa) (41.4.0)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.6/dist-packages (from flask-cors->cdqa) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->cdqa) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask->cdqa) (1.1.1)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert->cdqa) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.7 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert->cdqa) (1.13.7)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert->cdqa) (0.9.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert->cdqa) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert->cdqa) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert->cdqa) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert->cdqa) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.7->boto3->pytorch-pretrained-bert->cdqa) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-f-pzt-5O090",
        "colab_type": "code",
        "outputId": "d114a11b-7686-42e2-b73d-9bb84434ebfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "! git clone https://github.com/sumehta/question-generation.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'question-generation'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 91 (delta 16), reused 0 (delta 0), pack-reused 64\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Cg-rViZWUx",
        "colab_type": "code",
        "outputId": "d5c217c5-2c9d-4041-b0e3-03e15f4ed9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/question-generation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/question-generation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuM3JvH4ZXD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip QuestionGeneration.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzOjOw8f0hyg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc249099-e658-4040-d20b-9cda47e6e8ae"
      },
      "source": [
        "cd /content/"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyTkybvxO2AY",
        "colab_type": "code",
        "outputId": "691ccd78-37a7-41fb-cc37-6c769e26cb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!git clone https://github.com/kushalj001/FNC"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'FNC'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 29 (delta 4), reused 24 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (29/29), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NamQ15TOYQgG",
        "colab_type": "code",
        "outputId": "b7dc838c-5397-4b34-d77b-6bd0e35e72d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "neuralcoref.add_to_pipe(nlp)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f2264e31780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2av5ZwTbsfcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dktWUbapOqOP",
        "colab_type": "code",
        "outputId": "d54512ca-0858-4128-cf6c-125beacea635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import requests\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "from bs4 import BeautifulSoup\n",
        "import newspaper\n",
        "\n",
        "# import nltk\n",
        "# #nltk.download(\"all\")\n",
        "# from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from ast import literal_eval\n",
        "from cdqa.utils.download import download_model\n",
        "from cdqa.pipeline.cdqa_sklearn import QAPipeline\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "\n",
        "# Download models\n",
        "download_model(model='bert-squad_1.1', dir='./models')\n",
        "\n",
        "# Loading QAPipeline with CPU version of BERT Reader pretrained on SQuAD 1.1\n",
        "cdqa_pipeline = QAPipeline(reader='models/bert_qa_vCPU-sklearn.joblib')\n",
        "\n",
        "\n",
        "from question_generation.question_generator import QuestionGenerator\n",
        "\n",
        "qdf=pd.read_csv(\"/content/questiontable.csv\",encoding='latin')\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading trained model...\n",
            "bert_qa_vCPU-sklearn.joblib already downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXLw218CxjoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuKL_f6Ixjrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc=\"\"\"An Israeli company based in the northern coastal city of Ceasarea has developed a revolutionary minimal technique to freeze cancerous tumors that only requires a local anesthetic.\n",
        "\n",
        "It is already treating thousands of women with breast cancer worldwide within hospitals and doctors’ offices, including in Japan, Ausralia, Germany and Spain. A clinical trial is currently being performed in the United States.\n",
        "\n",
        "Vice President of Icecure Medical Elizabeth Sadka explains that the method is meant “to replace the surgery for a specific population that we can offer to treat in a minimal invasive operation.”\n",
        "\n",
        "A solution at a temperature of -274 degrees Fahrenheit is injected into the cancerous tumor to freeze it, rendering it inactive and enabling the body to dispense with it as it does with other dead cells naturally within a few weeks.\n",
        "\n",
        "Only a minimal scar remains in the tissue while no other trace is left on the remainder of the body or organs.\n",
        "\n",
        "Most patients can be treated within one hour and in some cases even within just ten to fifteen minutes, allowing them to go home the same day or the following day.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN9Pxu4qx7im",
        "colab_type": "code",
        "outputId": "5243301e-7348-408a-e0a2-02fc87b5079d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An Israeli company based in the northern coastal city of Ceasarea has developed a revolutionary minimal technique to freeze cancerous tumors that only requires a local anesthetic.\\n\\nIt is already treating thousands of women with breast cancer worldwide within hospitals and doctors’ offices, including in Japan, Ausralia, Germany and Spain. A clinical trial is currently being performed in the United States.\\n\\nVice President of Icecure Medical Elizabeth Sadka explains that the method is meant “to replace the surgery for a specific population that we can offer to treat in a minimal invasive operation.”\\n\\nA solution at a temperature of -274 degrees Fahrenheit is injected into the cancerous tumor to freeze it, rendering it inactive and enabling the body to dispense with it as it does with other dead cells naturally within a few weeks.\\n\\nOnly a minimal scar remains in the tissue while no other trace is left on the remainder of the body or organs.\\n\\nMost patients can be treated within one hour and in some cases even within just ten to fifteen minutes, allowing them to go home the same day or the following day.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wGVWjNqxju8",
        "colab_type": "code",
        "outputId": "7a93d5dc-9050-43f5-c594-2136535df598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        " for para in doc.split(\"\\n\"):\n",
        "        if(para):\n",
        "        \n",
        "          print(\"*\",para)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* An Israeli company based in the northern coastal city of Ceasarea has developed a revolutionary minimal technique to freeze cancerous tumors that only requires a local anesthetic.\n",
            "* It is already treating thousands of women with breast cancer worldwide within hospitals and doctors’ offices, including in Japan, Ausralia, Germany and Spain. A clinical trial is currently being performed in the United States.\n",
            "* Vice President of Icecure Medical Elizabeth Sadka explains that the method is meant “to replace the surgery for a specific population that we can offer to treat in a minimal invasive operation.”\n",
            "* A solution at a temperature of -274 degrees Fahrenheit is injected into the cancerous tumor to freeze it, rendering it inactive and enabling the body to dispense with it as it does with other dead cells naturally within a few weeks.\n",
            "* Only a minimal scar remains in the tissue while no other trace is left on the remainder of the body or organs.\n",
            "* Most patients can be treated within one hour and in some cases even within just ten to fifteen minutes, allowing them to go home the same day or the following day.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IflodKtV9lHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def google_search(query):\n",
        "    try: \n",
        "        from googlesearch import search \n",
        "    except ImportError:  \n",
        "        print(\"No module named 'google' found\") \n",
        " \n",
        "    List=[]  \n",
        "    for j in search(query, tld=\"co.in\", num=10, stop=10, pause=2): \n",
        "        List.append(j) \n",
        "    return List\n",
        "\n",
        "  \n",
        "def scrapped_dcos(links):\n",
        "\n",
        "    documents=[]\n",
        "    flag=0\n",
        "\n",
        "    URL=[]\n",
        "    for i in links:\n",
        "        article = newspaper.Article(i, language='en')\n",
        "        try:\n",
        "\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            documents.append(article.text)\n",
        "            URL.append([i])\n",
        "\n",
        "        except:\n",
        "            flag=1\n",
        "            \n",
        "    return(documents,URL)\n",
        "  \n",
        "  \n",
        "def getresolvedparas(documents,URL):\n",
        "\n",
        "    pattern='\\[.*?\\]'\n",
        "    pattern1='\\(.*?\\)'\n",
        "\n",
        "    paragraphs=[]\n",
        "    linklist=[]\n",
        "    \n",
        "\n",
        "    for url,doc in zip(URL,documents):\n",
        "      #print(\"docs\",doc)\n",
        "      for para in doc.split(\"\\n\"):\n",
        "        #print(\"paras\",para)\n",
        "        if(para): \n",
        "\n",
        "          para = re.sub(pattern, '', para)\n",
        "          para = re.sub(pattern1, '', para)\n",
        "          para = para.replace('\\t', ' ').replace('\\xa0',' ').replace(\".\",\". \").replace(\" , , \",\",\").replace(\", ,\",\",\")\n",
        "          para = ' '.join(para.split())\n",
        "\n",
        "          doc = nlp(para)\t\n",
        "          \n",
        "\n",
        "          #paragraphs.append([doc._.coref_resolved])\n",
        "          \n",
        "          paragraphs.append([para])\n",
        "          \n",
        "          linklist.extend(url)\n",
        "          \n",
        "    \n",
        "    \n",
        "    return(paragraphs,linklist)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4d3c8c73-3567-4c1b-dd9c-905a3c00a9a3",
        "id": "ri9ntsvjW8dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "factcheck(\"millennials are not buying cars any more and this change in thier mindset is the reason for automobile crisis\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('millennials are not buying cars any more and this change in thier mindset is the reason for automobile crisis', '“Millennials might be buying cars but sharing them more often, or they may have cars they use on weekends only and therefore invest less in it, because they’re going to use ride-share during the week,” Jillson said. “The place the vehicle is in the mindset of younger consumers is shifting because they are being offered more opportunities and more options. ”', 'https://www.mcall.com/business/mc-biz-cars-millennials-20181112-story.html', 'Agree')\n",
            "('millennials are not buying cars any more and this change in thier mindset is the reason for automobile crisis', '“Millennials might be buying cars but sharing them more often, or they may have cars they use on weekends only and therefore invest less in it, because they’re going to use ride-share during the week,” said Jillson. “The place the vehicle is in the mindset of younger consumers is shifting because they are being offered more opportunities and more options. ”', 'https://www.chicagotribune.com/business/ct-biz-young-adults-cars-attitudes-20181106-story.html', 'Agree')\n",
            "('millennials are not buying cars any more and this change in thier mindset is the reason for automobile crisis', \"Also Read | Finance Minister Nirmala Sitharaman blames millennials for car sale slump, says they aren't buying cars\", 'https://www.indiatoday.in/india/story/done-with-your-memes-nirmala-sitharaman-was-quoting-a-global-study-for-automobile-crisis-1598391-2019-09-12', 'Agree')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGMa4L4IEAf0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AVI2M5CWXj0",
        "colab_type": "code",
        "outputId": "d87e182e-6e5a-4d35-89ca-d3cd90c666c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc=nlp(\"modi is the prime minister of India. He was also the chiefminister of Gujarat\")\n",
        "print([doc._.coref_resolved])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['modi is the prime minister of India. modi was also the chiefminister of Gujarat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzBONOSPtZZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def factcheck(claim):\n",
        "  \n",
        "    answers=[]\n",
        "\n",
        "    links=google_search(str(claim))\n",
        "    documents,URL=scrapped_dcos(links)\n",
        "    paragraphs,linklist=getresolvedparas(documents,URL)\n",
        "\n",
        "    import pandas as pd\n",
        "    df= pd.DataFrame(columns=['title','paragraphs'])\n",
        "\n",
        "    df['paragraphs']=paragraphs\n",
        "    df['title']=linklist\n",
        "    \n",
        "\n",
        "    # Fitting the retriever to the list of documents in the dataframe\n",
        "    cdqa_pipeline.fit_retriever(df)\n",
        "    \n",
        "    #question_list1,question_list2=qans_generator(claim)\n",
        "    \n",
        "    #print(\"printing the list of questions\",question_list1,question_list2)\n",
        "    \n",
        "    answers.append(get_results(claim))\n",
        "    \n",
        "    \n",
        "#     for i in question_list1:\n",
        "      \n",
        "#         answers.append(get_results(claim,i[\"Q\"],i[\"A\"]))\n",
        "        \n",
        "    \n",
        "#     for i in question_list2:\n",
        "      \n",
        "      \n",
        "#         answers.append(get_results(claim,i[0],i[1]))\n",
        "    \n",
        "   \n",
        "    for i in answers:\n",
        "      \n",
        "      for j in i:\n",
        "        \n",
        "        print(j)\n",
        "            \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCGHNuY79bCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def elmo_vectors(x):\n",
        "  \n",
        "    embeddings = elmo(x,signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.tables_initializer())\n",
        "        return sess.run(embeddings)\n",
        "\n",
        "def simscore(str1,str2,ind1,ind2):\n",
        "  \n",
        "  v1=elmo_vectors([str1])\n",
        "  v2=elmo_vectors([str2])\n",
        "  \n",
        "  return(cosine_similarity(v1[0,ind1,:].reshape(1,1024), v2[0,ind2,:].reshape(1,1024)).flatten()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmvNFFMC9bEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generatequestion(text):\n",
        "  \n",
        "    tokenizedsent=[]\n",
        "\n",
        "    qalist=[]\n",
        "\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    for token in doc:\n",
        "        tokenizedsent.append(token.text)\n",
        "        \n",
        "    for ent in doc.ents:\n",
        "      \n",
        "         #print(ent,ent.label_)\n",
        "        \n",
        "        if(ent.label_==\"NORP\"): \n",
        "\n",
        "            ind1=tokenizedsent.index(ent.text)\n",
        "            sentence= ' '.join(token for token in tokenizedsent)\n",
        "\n",
        "\n",
        "            s1=simscore(sentence,\"congress is a political group\",ind1,3)\n",
        "            s2=simscore(sentence,\"i am a global citizen\",ind1,4)\n",
        "            s3=simscore(sentence,\"christianity is a religion\",ind1,3)\n",
        "\n",
        "            if((s1>s2) & (s1>s3)):\n",
        "\n",
        "                qalist.append((ent.text,text.replace(ent.text,\"which political party\")))\n",
        "\n",
        "            elif((s2>s1) & (s2>s3)):\n",
        "\n",
        "                qalist.append((ent.text,text.replace(ent.text,\"which country\")))\n",
        "\n",
        "            else:\n",
        "\n",
        "                qalist.append((ent.text,text.replace(ent.text,\"which religion\")))\n",
        "\n",
        "        else:                   \n",
        "\n",
        "            qalist.append((text.replace(ent.text,list(qdf.loc[qdf.type==ent.label_,\"question\"])[0]),ent.text))\n",
        "            \n",
        "    \n",
        "    return(qalist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GYiZ0Czn-CTo",
        "colab": {}
      },
      "source": [
        "def qans_generator(claim):\n",
        "  \n",
        "\n",
        "  q  = QuestionGenerator()\n",
        "  \n",
        "  question_list1 = q.generate_question(claim)\n",
        "  \n",
        "  question_list2 = generatequestion(claim)\n",
        "  \n",
        "  \n",
        "  return(question_list1,question_list2)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dentc3L14mo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_results(claim):\n",
        "  \n",
        "  \n",
        "  #prediction=cdqa_pipeline.predict(query=question, n_predictions=3)\n",
        "  \n",
        "  prediction=cdqa_pipeline.predict(query=claim, n_predictions=3)\n",
        "  \n",
        "  #answer=[]\n",
        "  stance=[]\n",
        "  paragraphs=[]\n",
        "\n",
        "  for i in range(0,3):\n",
        "\n",
        "    #answer.append(format(prediction[i][0]))  #answer\n",
        "    website =format(prediction[i][1])  #website\n",
        "    para = format(prediction[i][2])  #paragraph\n",
        "    \n",
        "    paragraphs.append(para)\n",
        "    \n",
        "    stance.append(getstance(claim,para,website))\n",
        "        \n",
        "    \n",
        "#   for i in range(0,3):\n",
        "    \n",
        "#     print(\"\\n\")\n",
        "    \n",
        "#     print(\"question \",claim)\n",
        "#     print(\"answer \",format(prediction[i][0]))  #answer\n",
        "#     print(\"website \",format(prediction[i][1]))  #website\n",
        "#     print(\"paragraph \",format(prediction[i][2]))  #paragraph\n",
        " \n",
        "  return(stance)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x14IwNaWwSJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_results1(claim,question,actualanswer):\n",
        "  \n",
        "  \n",
        "  #prediction=cdqa_pipeline.predict(query=question, n_predictions=3)\n",
        "  \n",
        "  prediction=cdqa_pipeline.predict(query=claim, n_predictions=3)\n",
        "  \n",
        "  #answer=[]\n",
        "  stance=[]\n",
        "  paragraphs=[]\n",
        "\n",
        "  for i in range(0,3):\n",
        "\n",
        "    #answer.append(format(prediction[i][0]))  #answer\n",
        "    website =format(prediction[i][1])  #website\n",
        "    para = format(prediction[i][2])  #paragraph\n",
        "    \n",
        "    paragraphs.append(para)\n",
        "    \n",
        "    stance.append(getstance(claim,para,website))\n",
        "        \n",
        "    \n",
        "#   for i in range(0,3):\n",
        "    \n",
        "#     print(\"\\n\")\n",
        "    \n",
        "#     print(\"question \",claim)\n",
        "#     print(\"answer \",format(prediction[i][0]))  #answer\n",
        "#     print(\"website \",format(prediction[i][1]))  #website\n",
        "#     print(\"paragraph \",format(prediction[i][2]))  #paragraph\n",
        " \n",
        "  return(claim,paragraphs,stance)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oemNLxJ1c86-",
        "colab_type": "code",
        "outputId": "ac0f6e4f-55d3-4f0c-dbb0-5f17be9a4435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "getstance(\"rajni has affair with silk smitha\",\"Strongest among the rumours from Rajini's heydays in films is that he was romantically involved with Silk Smitha\",\"www.rumours.com\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing paragraph in get stance function Strongest among the rumours from Rajini's heydays in films is that he was romantically involved with Silk Smitha\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('www.rumours.com', 'Unrelated')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL8mHOyHdyeV",
        "colab_type": "code",
        "outputId": "584b2968-7a4b-4478-fab0-620c0f887f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predict_stance_for_sources('i never came yesterday',\"Strongest among the rumours from Rajini's heydays in films, is that he was romantically involved with Silk Smitha.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Stance': 'Unrelated'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fnduvoBMuzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getstance(claim,para,website):\n",
        "  \n",
        "  stance=predict_stance_for_sources(claim,para)\n",
        "     \n",
        "  return((claim,para,website,stance['Stance']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gz7POV2gEOH",
        "colab_type": "code",
        "outputId": "09b07559-bdc1-46e5-8ff8-570012a12db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGp0Uc2RP_NV",
        "colab_type": "code",
        "outputId": "ad5e53ef-02c1-4f9e-9eb0-a0079e4a9a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "label_dict = {0:'Agree',1:'Disagree',2:'Discuss',3:'Unrelated'}\n",
        "model = load_model('/content/FNC/keras_model_updated')\n",
        "model._make_predict_function()\n",
        "count_vectorizer = pickle.load(open('/content/FNC/count_vectorizer1.pk','rb'))\n",
        "tfidf_vectorizer = pickle.load(open('/content/FNC/tfidf_vectorizer.pk','rb'))\n",
        "\n",
        "#print(count_vectorizer)\n",
        "#print(tfidf_vectorizer)\n",
        "#print(model.summary())\n",
        "\n",
        "def calculate_tfidf_score(headline,body):\n",
        "    '''Calculates cosine similarity between the headline and body. '''\n",
        "\n",
        "    headline_features = tfidf_vectorizer.transform([headline]).toarray()\n",
        "    body_features = tfidf_vectorizer.transform([body]).toarray()\n",
        "    cosine_similarity  = np.dot(headline_features,body_features.T)\n",
        "\n",
        "    return cosine_similarity[0][0]\n",
        "\n",
        "def create_input_for_model(headline,body,tfidf_score):\n",
        "    '''Creates input for the model.'''\n",
        "\n",
        "    headline_features = count_vectorizer.transform([headline]).toarray()\n",
        "    body_features = count_vectorizer.transform([body]).toarray()\n",
        "    input_feature = np.column_stack((headline_features,tfidf_score,body_features))\n",
        "\n",
        "    return input_feature[0]\n",
        "\n",
        "  \n",
        "def predict_stance_for_sources(headline,body):\n",
        "    '''Predict stance for events fetched from other sources similar to the given headline.'''\n",
        "\n",
        "    tfidf_score = calculate_tfidf_score(headline=headline,body=body)\n",
        "    input = create_input_for_model(headline=headline,body=body,tfidf_score=tfidf_score)\n",
        "\n",
        "    input = input.reshape((1,-1))\n",
        "    prediction = model.predict(input)\n",
        "    output = np.argmax(prediction)\n",
        "    stance = label_dict[output]\n",
        "    response = {'Stance':stance}\n",
        "\n",
        "    return response\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.19.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN673WElzGgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer=[]\n",
        "websites=[]\n",
        "paras=[]\n",
        "\n",
        "\n",
        "for i in range(0,n_predictions):\n",
        "\n",
        "  answer.append(format(prediction[i][0]))  #answer\n",
        "  websites.append(format(prediction[i][1]))  #website\n",
        "  paras.append(format(prediction[i][2]))  #paragraph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jedfaOlWtlG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1rFzRzotlJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpAQV1gyLj83",
        "colab_type": "code",
        "outputId": "7e516591-ccb6-4a5c-f63b-7538ae5b2ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc=nlp(\"modi is the prime minister of India. He was also the chiefminister of Gujarat\")\n",
        "print([doc._.coref_resolved])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['modi is the prime minister of India. modi was also the chiefminister of Gujarat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCnZJXbaLFMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paragraphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4NxBKKr_GL_",
        "colab_type": "code",
        "outputId": "c7a9aa22-7c67-45d6-b2fa-f2cde4957b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Sending a question to the pipeline and getting prediction\n",
        "\n",
        "\n",
        "generatequestion(\"Rajini has affair with silk smitha\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rajini ORG\n",
            "question list 2 [('Rajini', 'which organization has affair with silk smitha')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Rajini', 'which organization has affair with silk smitha')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzFq_80DjYu-",
        "colab_type": "code",
        "outputId": "64daf5b4-e44d-4e0a-8802-858e4215f23d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(\"Rajini has affair with silk smitha\")\n",
        "       \n",
        "for ent in doc.ents:\n",
        "       print(ent,ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rajini ORG\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}